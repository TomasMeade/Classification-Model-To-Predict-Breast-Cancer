---
title: Classification Model Using Basic Patient Information and Bloodwork to Predict Breast Cancer
author: "Tomas Meade"
header-includes:
    - \usepackage{setspace}\doublespacing
    - \usepackage{float}
    - \floatplacement{figure}{H}
output:
  pdf_document: default
  bookdown::html_document2: default
  bookdown::pdf_document2: default
---


\newpage

# Abstract

### Introduction

This paper focuses on creating and assessing a logistic regression classification model to predict the risk of breast cancer using easily attainable patient information and bloodwork samples. 

### Methods

The variables in the dataset consist of basic patient information and blood work analysis for all 116 observations. The variables are the age, BMI, glucose, insulin, HOMA, leptin, adiponectin, resistin (ng/mL), MCP-1(pg/dL) and a variable that classifies if a participant has breast cancer or not. Logistic regression was used to create the model and it was assessed by calculating the concordance index, sensitivity and specificity. 

### Results

The final logistic regression model consisting of BMI, glucose, insulin, HOMA, resistin and MCP-1 had concordance index of 0.73. At a threshold value of $0.50$, the sensitivity was 59% and the specificity was 74%. Lowering the threshold value to $0.44$ achieved a sensitivity of 74% and a specificity of 53%.


### Conclusion

These results suggest that there is some evidence to suggest that models using easily attainable parameters could have potential in predicting the risk of breast cancer in women. 


# Introduction

This project sought to evaluate if easily attainable patient information and data which can be gathered in routine blood analysis are effective in predicting breast cancer. The outcome of interest was the binary variable categorizing whether a patient has breast cancer or not. The predictor variables are age, BMI, Glucose, Insulin, HOMA, Leptin, Adiponectin, Resistin and MCP-1. The main reason these variables were chosen is because they are attained easily and inexpensively while also having ties to breast cancer. The age and BMI of a patient are very basic patient data that a hospital would have on record for most likely all their patients. The rest of the variables can be obtained in a routine blood analysis. These variables have also been linked to breast in previous literature. So, overall the variables were chosen by balancing their attainability and their known association with breast cancer.


# Methods

The dataset consisted of 116 observations made up of 64 cancer patients and 52 healthy controls. The source of the data was a study done at the Gynaecology Department of the University Hospital Centre of Coimbra (CHUC) between 2009 and 2013. Women who were recently diagnosed with breast cancer at CHUC were recruited and all samples were collected before surgery and treatment. Healthy females were then also recruited and enrolled in the study as controls. All participants were free from any infection or other acute diseases or comorbidities at the time of enrollment in the study and never received any prior cancer treatment. The variables in the dataset consisted of basic patient information and blood work analysis for all 116 observations. The basic patient information was the age (years) and BMI (kg/m2) of each participant and a variable that classifies whether a participant has breast cancer or not. The bloodwork information consists of glucose (mg/dL), insulin (µU/mL) HOMA ((glucose in mmol/L x insulin in mIU/mL)/22.5), leptin (ng/mL), adiponectin (µg/mL), resistin (ng/mL), and MCP-1(pg/dL). All blood samples were collected at the same time of the day after an overnight fasting.  


To begin the analysis, box plots were created in order to examine the relationship between some of the predictor variables and the outcome variable which classifies whether a patient has breast cancer or not. Then the data was split up randomly into training and test groups with 60% of the data in the training set and 40% of the data in the test set. The variables and their relationship to the outcome variable were examined individually in single variable logistic regression models. Variable selection was then done with the training dataset using the best subsets method to determine the optimal set of predictor variables. The criterion used to determine the optimal subset was the BIC value of each model. With the final logistic model constructed, the c-index was calculated on the test data set to measure the concordance rate for the model. Then both the sensitivity and specificity of the model were calculated using the test set with various values of the threshold value c. The value of c was analyzed by focusing on a higher sensitivity rate in order to prioritize identifying anyone at risk of having breast cancer. This was done by creating an ROC curve to visualize the trade off between the sensitivity and specificity rate. 
   

```{r, include=FALSE}

#Import packages
library(kableExtra)
library(assertive, warn.conflicts = FALSE)
library(tidyverse)
library(broom)
library(knitr)

#Read in data
data <- read_csv("dataR2.csv")

#Classify 2 as TRUE for breast cancer patients
data$Classification=data$Classification=="2"

#Export data
write.csv(data, "dataStats242Final.csv")

```


```{r, include=FALSE}

#Prevalence of breast cancer in dataset
nrow(data)
sum(data$Classification == 0)
sum(data$Classification == 1)
sum(data$Classification == 1)/nrow(data)

```


```{r, include=FALSE}

#Single variable logistic regression
fit.age <- glm(Classification ~ Age, data = data)
summary(fit.age)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.BMI <- glm(Classification ~ BMI, data = data)
summary(fit.BMI)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.glucose <- glm(Classification ~ Glucose, data = data)
summary(fit.glucose)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.insulin <- glm(Classification ~ Insulin, data = data)
summary(fit.insulin)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.HOMA <- glm(Classification ~ HOMA, data = data)
summary(fit.HOMA)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.leptin <- glm(Classification ~ Leptin, data = data)
summary(fit.leptin)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.Adiponectin <- glm(Classification ~ Adiponectin, data = data)
summary(fit.Adiponectin)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.resistin <- glm(Classification ~ Resistin, data = data)
summary(fit.resistin)


```


```{r, include=FALSE}

#Single variable logistic regression
fit.MCP1 <- glm(Classification ~ MCP.1, data = data)
summary(fit.MCP1)


```


```{r, include=FALSE}


#Create easily accesable summary statistics for SLR models
fit.ageM <- tidy(fit.age)

fit.AdiponectinM <- tidy(fit.Adiponectin)

fit.BMIM <- tidy(fit.BMI)

fit.glucoseM <- tidy(fit.glucose)

fit.HOMAM <- tidy(fit.HOMA)

fit.insulinM <- tidy(fit.insulin)

fit.leptinM <- tidy(fit.leptin)

fit.resistinM <- tidy(fit.resistin)

fit.MCP1M <- tidy(fit.MCP1)

#Create dataframe of summary statistics for LR models

Variable <- c('Age','Adiponectin','BMI', 'Glucose', 'HOMA', 'Insulin', 'Leptin', 'Resistin', 'MCP1')

           
Coefficient <- c(round(as.numeric(fit.ageM[2, "estimate"]), digits = 5), 
                 round(as.numeric(fit.AdiponectinM[2, "estimate"]), digits = 5), 
                 round(as.numeric(fit.BMIM[2, "estimate"]), digits = 5), 
                 round(as.numeric(fit.glucoseM[2, "estimate"]), digits = 5),
                 round(as.numeric(fit.HOMAM[2, "estimate"]), digits = 5),
                 round(as.numeric(fit.insulinM[2, "estimate"]), digits = 5),
                 round(as.numeric(fit.leptinM[2, "estimate"]), digits = 5),
                 round(as.numeric(fit.resistinM[2, "estimate"]), digits = 5),
                 round(as.numeric(fit.MCP1M[2, "estimate"]), digits = 5))


P_value <- c(round(as.numeric(fit.ageM[2, "p.value"]), digits = 3), 
             round(as.numeric(fit.AdiponectinM[2, "p.value"]), digits = 3), 
             round(as.numeric(fit.BMIM[2, "p.value"]), digits = 3), 
             '2.05e-05',
             round(as.numeric(fit.HOMAM[2, "p.value"]), digits = 3),
             round(as.numeric(fit.insulinM[2, "p.value"]), digits = 3),
             round(as.numeric(fit.leptinM[2, "p.value"]), digits = 3),
             round(as.numeric(fit.resistinM[2, "p.value"]), digits = 3),
             round(as.numeric(fit.MCP1M[2, "p.value"]), digits = 3))




lr.df <- data.frame(Variable, Coefficient, P_value)

names(lr.df)[names(lr.df) == "P_value"] <- "p-value"

```



```{r, include=FALSE}

## 1. Split dataset into a training and a test set using a 60-40 split
set.seed(16)
train <- data %>% sample_frac(0.6)
test <- data %>% setdiff(train)
nrow(test)
nrow(train)

```



```{r, include=FALSE}

#Find best model using best subsets method
library(leaps)

subsets <- regsubsets(Classification ~ ., data = train, nbest = 1)
subsets.sum <- summary(subsets)
names(subsets.sum)

```


```{r, include=FALSE}

d <- data.frame(cbind(subsets.sum$which, BIC = subsets.sum$bic))
d


```



```{r, include=FALSE}

numTerms <- which.min(d$BIC)
numTerms

```



```{r, include=FALSE}

coef(subsets, numTerms)


```



```{r, include=FALSE}

#Final model
final.fit <- glm(Classification ~ BMI + Glucose + Insulin + HOMA + Resistin, data = train)
summary(final.fit)

```



```{r, include=FALSE}


#Create easily accesable summary statistics for SLR models
final.fitM <- tidy(final.fit)

#Create dataframe of summary statistics for final model

VariableF <- c('BMI', 'Glucose', 'Insulin', 'HOMA', 'Resistin')

           
CoefficientF <- c(round(as.numeric(final.fitM[2, "estimate"]), digits = 5), 
                 round(as.numeric(final.fitM[3, "estimate"]), digits = 5), 
                 round(as.numeric(final.fitM[4, "estimate"]), digits = 5), 
                 round(as.numeric(final.fitM[5, "estimate"]), digits = 5),
                 round(as.numeric(final.fitM[6, "estimate"]), digits = 5))


P_valueF <- c(round(as.numeric(final.fitM[2, "p.value"]), digits = 5),
             '4.36e-05',
             round(as.numeric(final.fitM[4, "p.value"]), digits = 5), 
             round(as.numeric(final.fitM[5, "p.value"]), digits = 5), 
             round(as.numeric(final.fitM[6, "p.value"]), digits = 5))



Flr.df <- data.frame(VariableF, CoefficientF, P_valueF)

names(Flr.df)[names(Flr.df) == "P_valueF"] <- "p-value"
names(Flr.df)[names(Flr.df) == "VariableF"] <- "Variable"
names(Flr.df)[names(Flr.df) == "CoefficientF"] <- "Coefficient"

```


```{r, include=FALSE}

#Calculate c-index
library(Hmisc)
phat <- predict(final.fit, test, type = "resp")
rcorr.cens(S = test$Classification, x = phat)[["C Index"]]

```


```{r, include=FALSE}

#Calculate sensitivity and specificity
sensspec <- function(c, obsY, phat)
{
  predY <- (phat>=c)
  tab <- table(predY, obsY)
  sens <- mean(predY[obsY==1]) ## sensitivity, true positive
  spec <- mean((1-predY)[obsY==0]) ## specificity, true negative
  return(list(tab = tab, stats = c(sens=sens,spec=spec)))
}


sensspec(0.5, obsY = test$Classification, phat = predict(final.fit,test, type = "resp"))

```


```{r, include=FALSE}

sensspec(0.44, obsY = test$Classification, phat = predict(final.fit,test, type = "resp"))

```


# Results


```{r, echo=FALSE, fig.width=5,fig.height=4, fig.pos="H", fig.cap="\\label{fig:figs} Boxplot showing the observed difference between glucose levels of healthy controls versus breast cancer patients."}

#Boxplots for Predictor Variables
boxplot <- data %>% ggplot(aes(x = Classification, y = Glucose, group = Classification, color = Classification)) + geom_boxplot() + theme_bw() +
  labs(title="Glucose by Breast Cancer Classification", x="Breast Cancer",
       y="Glucose", color="Breast Cancer Classification")
boxplot

```



From the basic exploratory analysis, the glucose, HOMA and insulin levels of healthy controls versus breast cancer patients seemed to have the most obvious observed difference. This was confirmed in the single variable logistic regression models since glucose, HOMA and insulin had the strongest association to breast cancer with $p-values$ that would be significant at any reasonable alpha-level (see table 1).


The final logistic regression model consisted of five parameters. The final set of variables were BMI, glucose, insulin, HOMA and resistin. Using an alpha-level of $0.05$, each variable was significant (see table 2). The concordance index of the final model was 0.73. At a threshold value of $0.50$, the sensitivity was 59% and the specificity was 74%. To prioritize sensitivity, lowering the threshold value to $0.44$ achieved a sensitivity of 74% and a specificity of 53%. The trade off between sensitivity and specificity is visualized in the ROC curve in figure 2. 


The concordance index of 0.73 shows some indication that the model was effective in discriminating between healthy controls and breast cancer patients. With a threshold value of $0.44$, the model shows potential to have a strong true positive rate correctly identifying participants with breast cancer while still maintaining a true negative rate of over 50%.

\newpage


```{r tab1, echo=FALSE}

#Create table of summary statistics for SLR models
kable(lr.df, caption = 'Each row represents a logistic regression model with the stated predictor variable and the outcome variable which classifies whether a patient has cancer or not. Each predictor variable is listed along with its associated coefficient and p-value.') %>%
  kable_styling(latex_options = "hold_position")

```


```{r tab2, echo=FALSE}

#Create table of summary statistics for SLR models
library(knitr)
kable(Flr.df, caption = 'Each predictor variable is listed along with its associated coefficient and p-value in the final logistic regression model.') %>%
  kable_styling(latex_options = "hold_position")

```




```{r, fig1, echo=FALSE, fig.width=5,fig.height=4, fig.pos="H", fig.cap="\\label{fig:figs} ROC curve showing the trade of between sensitivity and specificity."}

#ROC Curve
calcSensSpec <- function(c, phat, Yobs)
{
  Ynew <- ifelse(phat >= c, 1, 0)
  sens <- mean(Ynew[Yobs==1]) ## true positive
  spec <- 1-mean(Ynew[Yobs==0]) ## true negative
  return(c(sens=sens, spec=spec))
}

plotROC <- function(fit, Y, mytitle = "ROC Curve")
{
  cvec <- seq(0,1,0.001)
  sensSpecVec <- sapply(cvec, calcSensSpec, predict(final.fit, test, type = "resp"), Y)
  plot(1-sensSpecVec["spec",], sensSpecVec["sens",],
       ylab = "Sensitivity", xlab = "1-Specificity",
       main = mytitle)
}


plotROC(fit = final.fit, Y = test$Classification, mytitle = "ROC Curve for Breast Cancer Model")

```



# Discussion

These results are mostly consistent with previous work, but other studies have resulted in more promising results. The original study done at at CHUC constructed models that achieved sensitivity and specificity rates over 80%. This is most likely because the study done at CHUC was more in depth and thorough in its statistical analysis. This paper only uses a basic logistic regression model and uses the best subsets method to construct the final model without an extensive knowledge of the biology behind the parameters and their association with breast cancer. The BIC value was used as a criterion for variable selection in order to emphasize the theme of developing a simple model with only a small amount of parameters and penalize overfitting. However, using the BIC value could have penalized having more parameters to a degree that was too extreme, resulting in a over simplified model. The results of this paper are still promising since they reaffirm the potential for the effectiveness of using simple parameters to help predict the risk of breast, but they are incomplete and not entirely conclusive. 


# Conclusion

This paper conducts only a brief statistical analysis of the predictive capability of a logistic regression model to determine the risk of breast cancer. The analysis was done without an extensive knowledge of the parameters of interest or breast cancer. However, the results do align with previous literature in the field and suggest that only anthropocentric and bloodwork data can still be effective in predicting the risk of breast cancer. Further studies should be done to assess these results and whether or not they could be applied to the general population.


\newpage

# References

Assiri AM, Kamel HF. Evaluation of diagnostic and predictive value of serum adipokines: Leptin, resistin
\begin{itemize}
  \item[] and visfatin in postmenopausal breast cancer. Obes Res Clin Pract. 2015;10(4):442-53.
\end{itemize}


Cole KD, He HJ, Wang L. Breast cancer biomarker measurements and standards.
\begin{itemize}
  \item[] Proteomics Clin Appl. 2013;7(1–2):17–29.
\end{itemize}


Crisóstomo J, et al. Hyperresistinemia and metabolic dysregulation: the close 
\begin{itemize}
  \item[] crosstalk in obese breast cancer. Endocrine. 2016;53(2):433-42.
\end{itemize}


Kloten V, et al. Promoter hypermethylation of the tumor-suppressor genes ITIH5, DKK3, and RASSF1A
\begin{itemize}
  \item[] as novel biomarkers for blood-based breast cancer screening. Breast Cancer Res. 2013;15(1):R4.
\end{itemize}


Patrício, M., Pereira, J., Crisóstomo, J. et al. Using Resistin, glucose, age and BMI to predict the presence
\begin{itemize}
  \item[] of breast cancer. BMC Cancer 18, 29 (2018).
\end{itemize}

